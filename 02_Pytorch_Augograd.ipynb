{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916e1a4a",
   "metadata": {},
   "source": [
    "### AutoGrad in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9445cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograd : Automatic Differentiation in Python, with a focus on machine learning.\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "715e17f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "857c4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39ee43d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x value is : 3.0\n",
      "y value is : 9.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"x value is : {x}\")\n",
    "print(f\"y value is : {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76c1db51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y #tensor(9., grad_fn=<PowBackward0>), where grad_fn shows that this tensor is the result of a computation that involved gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9245043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward() # compute the gradients, dy/dx and store them in x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73334c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad # tensor(6.) which is the derivative of y with respect to x at x=3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d7419c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def dz_dx(x):\n",
    "    return 2*x*math.cos(x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37a6377e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.466781571308061"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_dx(3.0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da08c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "x = torch.tensor(3.0, requires_grad=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2e40a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd4da9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.sin(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "972abbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "960596fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d90d2b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4121, grad_fn=<SinBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b191fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc0277d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#Inputs \n",
    "x = torch.tensor(6.7) # Input feature\n",
    "y = torch.tensor(0.0) # Target Output\n",
    "\n",
    "w = torch.tensor(1.0) # Weight\n",
    "b = torch.tensor(0.0) # Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbabd124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross-entropy loss for scalar \n",
    "def binary_cross_entropy_loss(prediction, target):\n",
    "    epsilon = 1e-8  # To prevent log(0)\n",
    "    prediction = torch.clamp(prediction, epsilon, 1-epsilon)\n",
    "    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf6750cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value: 0.998770534992218\n",
      "Loss Value: 6.701176166534424\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass\n",
    "z = w * x + b  \n",
    "y_pred = torch.sigmoid(z)\n",
    "loss = binary_cross_entropy_loss(y_pred, y)\n",
    "print(f\"Predicted Value: {y_pred.item()}\")\n",
    "print(f\"Loss Value: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04f1a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivatives\n",
    "# 1. dL/d(y_pred) : Loss with respect to predicted output\n",
    "dloss_by_pred = (y_pred - y) / (y_pred * (1-y_pred))\n",
    "\n",
    "# dy_pred/dz : Prediction (y_pred) with respect to z (sigmoid derivative)\n",
    "dy_pred_dz = y_pred * (1- y_pred)\n",
    "\n",
    "# dz/dw and dz/db : z with respect to w and b \n",
    "dz_dw = x # since z = w*x + b\n",
    "dz_db = 1\n",
    "\n",
    "dL_dw = dloss_by_pred * dy_pred_dz * dz_dw\n",
    "dL_db = dloss_by_pred * dy_pred_dz * dz_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "672441bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Gradient of loss w.r.t weight (dw) : 6.691762447357178\n",
      "Manual Gradient of loss w.r.t bias (db) : 0.998770534992218\n"
     ]
    }
   ],
   "source": [
    "print(f\"Manual Gradient of loss w.r.t weight (dw) : {dL_dw}\")\n",
    "print(f\"Manual Gradient of loss w.r.t bias (db) : {dL_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd0c81c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will see the same using PyTorch Autograd where we don't have to compute gradients manually.\n",
    "# Setting requires_grad=True for w and b to track computations\n",
    "\n",
    "x = torch.tensor(6.7)\n",
    "y = torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7766ed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce13e4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5aac2190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3aeaf4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7000, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = w * x + b\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "535f2750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9988, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = torch.sigmoid(z)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef7a417f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7012, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = binary_cross_entropy_loss(y_pred, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ad05e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e528ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6918)\n",
      "tensor(0.9988)\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93c34028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3c7b9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6667, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (x ** 2).mean()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abca3513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6667, 1.3333, 2.0000])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward() # compute gradients\n",
    "x.grad # tensor([0.6667, 1.3333, 2.0000]) which is the derivative of y with respect to each element in x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8b50d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clearing grad\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd696e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x ** 2 \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50eabfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88741f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad # it has accumulated gradients, so it will be 4.0 from first backward and 4.0 from second backward making it 8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "797d6db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() # Clear the gradients, in place operation where _ indicates in-place operation ; meaning the original tensor is modified\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab6ac36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To stop tracking gradients, we can use torch.no_grad() context manager or .detach() method. also we can use .requires_grad_(False) to stop tracking gradients for a tensor.\n",
    "# this is necessary during inference phase to save memory and computations.\n",
    "\n",
    "x.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c05b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
