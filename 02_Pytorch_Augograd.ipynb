{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57112f9c",
   "metadata": {},
   "source": [
    "##### AgutoGrad\n",
    "Autograd automatically calculates derivatives, which are essential for optimization algorithms like gradient descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "312035e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def dy_dx(x):\n",
    "    return 2*x\n",
    "\n",
    "dy_dx(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1100449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "895cdddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.6145744834544478\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def dz_dx(x):\n",
    "    return 2 * x * math.cos(x**2)\n",
    "\n",
    "print(dz_dx(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9250304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "import torch \n",
    "\n",
    "x = torch.tensor(3.0, requires_grad = True) # Requires Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51e2c018",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaf49376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "666b18e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8bc0259",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward() # It wil calculate all the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c8c15fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad # it will print the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a033a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2\n",
    "# y = x ** 2, z = sin(y)\n",
    "\n",
    "import torch \n",
    "\n",
    "x = torch.tensor(3.0, requires_grad = True)\n",
    "\n",
    "y = x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e672fb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.sin(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb0ab8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is : 3.0.\n",
      " y is : 9.0.\n",
      " z is : 0.41211849451065063.\n"
     ]
    }
   ],
   "source": [
    "print(f\"x is : {x}.\\n y is : {y}.\\n z is : {z}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9acfb2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08f1315f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-5.4668)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f730190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3 \n",
    "# Perceptron for CGPA and placement prediction \n",
    "import torch \n",
    "\n",
    "x = torch.tensor(6.7) # Input feature\n",
    "y = torch.tensor(0.0) # Groud Truth\n",
    "\n",
    "w = torch.tensor(1.0) # Weight\n",
    "b = torch.tensor(0.0) # Bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdeeb2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross Entropy Loss for scalar\n",
    "def binary_cross_entropy_loss(prediction, target):\n",
    "    epsilon = 1e-8 # To prevent log(0)\n",
    "    prediction = torch.clamp(prediction, epsilon, 1 - epsilon)\n",
    "    return -(target * torch.log(prediction) + (1- target) * torch.log(1- prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da58f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "z = w * x + b\n",
    "y_pred = torch.sigmoid(z)\n",
    "\n",
    "# Compute Binary Cross-Entropy Loss\n",
    "loss = binary_cross_entropy_loss(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25a3d809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7012)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f21fd348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivatives:\n",
    "# 1. dL/d(y_pred): Loss with respect to the prediction (y_pred)\n",
    "dloss_dy_pred = (y_pred - y)/(y_pred * (1 - y_pred))\n",
    "\n",
    "# 2. dy_pred/dz = prediction (y_pred) with respect to z(sigmoid derivative)\n",
    "dy_pred_dz = y_pred * (1-y_pred)\n",
    "\n",
    "# 3. dz/dw and dz/db : z with respect to w and b\n",
    "dz_dw = x\n",
    "dz_db = 1\n",
    "\n",
    "dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n",
    "dL_db = dloss_dy_pred * dy_pred_dz * dz_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b86e2b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual radiant of loss w.r.t weight (dw) : 6.691762447357178\n",
      "Manual Gradient of loss w.r.t bias (db) : 0.998770534992218\n"
     ]
    }
   ],
   "source": [
    "print(f\"Manual radiant of loss w.r.t weight (dw) : {dL_dw}\")\n",
    "print(f\"Manual Gradient of loss w.r.t bias (db) : {dL_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65645073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do the same with torch \n",
    "\n",
    "x = torch.tensor(6.7)\n",
    "y = torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3ac5cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(1.0, requires_grad = True)\n",
    "b = torch.tensor(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "523c5e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "061c0e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d23e3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = w * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74801c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9988, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = torch.sigmoid(z)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "973ae064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7012, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = binary_cross_entropy_loss(y_pred, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a8b3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3e8c99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6918)\n",
      "tensor(0.9988)\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef18f418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another example\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20aa860c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6667, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (x**2).mean()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c34221fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b03658c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6667, 1.3333, 2.0000])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad # Now we are getting the 3 gradients which means, each element in the vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d903b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clearing Gradients, if we do not clear the gradiants then they get accumulate at the end.\n",
    "x = torch.tensor(2.0, requires_grad =True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3354b286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x ** 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "99312d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cb28a499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dae06d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() # doing inplace changes and making the grad = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are few ways of diabiling the gradiant tracking when it is necessary. Especially we do not need to track the gradients while testing the model as we won't be doing the back propagation for updating the weights\n",
    "# If we do not turn of the tracking then it will consume lot of memory when we deal with the larger tensors during the model testing.\n",
    "\n",
    "# Option 1 : requires_grad_(False)\n",
    "# Option 2 : detach()\n",
    "# Option 3 : torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eaf26de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad = True)\n",
    "y = x ** 2\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "00d7716e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = w * x + b\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fc933714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8808, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = torch.sigmoid(z)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a84957be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-5.8731, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = binary_cross_entropy_loss(y_pred, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2d89a0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "164bce7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad = True)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cd30f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y = x ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c26e9313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3e977506",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PROGRAMMING/Projects/GPU-pytorch/env/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PROGRAMMING/Projects/GPU-pytorch/env/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4bc14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
